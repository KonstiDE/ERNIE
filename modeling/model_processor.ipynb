{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "### Load model created by analyse_docs(...)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9b4827133d378744"
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "import os.path\n",
    "import fasttext\n",
    "import pickle as pkl\n",
    "import shutup\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from bertopic import BERTopic\n",
    "from transformers import MBartForConditionalGeneration, MBart50TokenizerFast\n",
    "\n",
    "shutup.please()\n",
    "\n",
    "base_path = \"/home/s371513/ernie/\"\n",
    "\n",
    "topic_model = BERTopic.load(os.path.join(base_path, \"model\"))\n",
    "model_path = hf_hub_download(repo_id=\"facebook/fasttext-language-identification\", filename=\"model.bin\")\n",
    "model = fasttext.load_model(model_path)\n",
    "\n",
    "tmodel = MBartForConditionalGeneration.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "tokenizer = MBart50TokenizerFast.from_pretrained(\"facebook/mbart-large-50-many-to-one-mmt\")\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T04:50:13.646613700Z",
     "start_time": "2024-02-28T04:50:06.784734Z"
    }
   },
   "id": "initial_id"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Label Topics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1f5a764be83b8ec8"
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "custom_topics = {}\n",
    "\n",
    "for i in range(len(topic_model.get_topic_info())):\n",
    "    s = \"\"\n",
    "    \n",
    "    for tup in topic_model.get_topic(i):    \n",
    "        try:\n",
    "            impact_factor = float(tup[1])\n",
    "            topic_keyword = tup[0]\n",
    "        except Exception as _:\n",
    "            impact_factor = float(tup[0])\n",
    "            topic_keyword = tup[1]\n",
    "            \n",
    "        encoded_hi = tokenizer(topic_keyword, return_tensors=\"pt\")\n",
    "        generated_tokens = tmodel.generate(**encoded_hi)\n",
    "        token = tokenizer.batch_decode(generated_tokens, skip_special_tokens=True)\n",
    "        s += str(token) + \" [\" + str(impact_factor) + \"]\\n\"\n",
    "    \n",
    "    topic = input(\"----------------------------\\nTopics already created: \\n{}\\n----------Topic {}----------\\n{}----------------------------\".format(set(custom_topics.values()), str(i), s))\n",
    "        \n",
    "    custom_topics[i] = topic\n",
    "\n",
    "with open(os.path.join(base_path, \"custom_topics.pkl\"), \"wb+\") as f:\n",
    "    pkl.dump(custom_topics, f)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T04:57:01.221163800Z",
     "start_time": "2024-02-28T04:56:31.917201500Z"
    }
   },
   "id": "ce6edb44c0b1dfa7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Match Topics"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89079ffdc522f441"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    if os.path.isfile(os.path.join(cfg.base_path(), \"custom_topics.pkl\")):\n",
    "        with open(os.path.join(cfg.base_path(), \"custom_topics.pkl\"), 'rb') as f:\n",
    "            print(\"Loading custom topics...\")\n",
    "            topic_dictionary = pkl.load(f)\n",
    "\n",
    "            print(\"Reading texts...\")\n",
    "            file_names, _ = read_text(\"preprocessed.txt\")\n",
    "\n",
    "            print(\"Applying topics to files in batches of {}...\".format(chunk_size))\n",
    "            file_chunks = [file_names[i:i + chunk_size] for i in range(0, len(file_names), chunk_size)]\n",
    "\n",
    "            c = 1\n",
    "            for file_chunk in file_chunks:\n",
    "                print(\"-\" * 16)\n",
    "                print(\"Batch {} of {}\".format(c, len(file_chunks)))\n",
    "                print(\"-\" * 16)\n",
    "                loop = tqdm(file_chunk)\n",
    "\n",
    "                corrupted_files = 0\n",
    "\n",
    "                for topic_index, doc_file in enumerate(loop):\n",
    "                    with open(os.path.join(cfg.gdelt_out(), doc_file), \"rb\") as d:\n",
    "                        try:\n",
    "                            document = pkl.load(d)\n",
    "                            d.close()\n",
    "                            document.topic_information = topic_dictionary[topic_index]\n",
    "                            document.save_document()\n",
    "\n",
    "                        except EOFError as _:\n",
    "                            corrupted_files += 1\n",
    "                            loop.set_postfix_str(\"Coruppted files: {}\".format(corrupted_files))\n",
    "\n",
    "                        d.close()\n",
    "                c += 1\n",
    "    else:\n",
    "        print(\"No file of custom topics found, execute label_topics() first!\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2024-02-28T03:14:22.121058500Z"
    }
   },
   "id": "11209d403dc53006"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
